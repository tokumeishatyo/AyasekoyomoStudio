import Foundation
@preconcurrency import AVFoundation
import CoreImage
import AppKit

// ‚òÖ‚òÖ‚òÖ ÊºîÂá∫ÊåáÁ§∫Êõ∏ („Å©„ÅÆÊôÇÈñì„Å´„Å©„Çì„Å™È°î„Çí„Åô„Çã„Åã) ‚òÖ‚òÖ‚òÖ
struct VideoScene: Sendable {
    let startTime: Double
    let endTime: Double
    let emotion: String // "happy", "angry", "sad", "neutral"
}

@MainActor
final class VideoExportManager: NSObject {
    
    static let shared = VideoExportManager()
    private override init() { super.init() }
    
    private let videoSize = CGSize(width: 1080, height: 1080)
    private let frameRate: Int32 = 30
    
    enum ExportError: LocalizedError {
        case failedToCreateAssetWriter, failedToCreateAudioFile, failedToReadAudioSamples, trackNotFound
        case exportFailed(String)
        
        var errorDescription: String? {
            switch self {
            case .failedToCreateAssetWriter: return "AssetWriter‰ΩúÊàêÂ§±Êïó"
            case .failedToCreateAudioFile: return "Èü≥Â£∞„Éï„Ç°„Ç§„É´Ë™≠ËæºÂ§±Êïó"
            case .failedToReadAudioSamples: return "Èü≥Â£∞„Çµ„É≥„Éó„É´Ë™≠ËæºÂ§±Êïó"
            case .trackNotFound: return "„Éà„É©„ÉÉ„ÇØ‰∏çÊòé"
            case .exportFailed(let msg): return "Êõ∏„ÅçÂá∫„ÅóÂ§±Êïó: \(msg)"
            }
        }
    }
    
    // ‚òÖ‚òÖ‚òÖ Â§âÊõ¥ÁÇπ: scenes (ÊåáÁ§∫Êõ∏„É™„Çπ„Éà) „ÇíÂèó„ÅëÂèñ„Çã„Çà„ÅÜ„Å´Â§âÊõ¥ ‚òÖ‚òÖ‚òÖ
    nonisolated func exportVideo(audioData: Data, scenes: [VideoScene]) async throws -> URL {
        print("üé• Export: ÈñãÂßã („Ç∑„Éº„É≥Êï∞: \(scenes.count))")
        
        let tempDir = FileManager.default.temporaryDirectory
        let uuid = UUID().uuidString
        let audioTempURL = tempDir.appendingPathComponent("\(uuid).mp3")
        let videoOutputURL = tempDir.appendingPathComponent("\(uuid).mp4")
        
        try? FileManager.default.removeItem(at: audioTempURL)
        try? FileManager.default.removeItem(at: videoOutputURL)
        try audioData.write(to: audioTempURL)
        
        let audioFile = try AVAudioFile(forReading: audioTempURL)
        let audioFormat = audioFile.processingFormat
        let audioFrameCount = AVAudioFrameCount(audioFile.length)
        let sampleRate = audioFormat.sampleRate
        let duration = Double(audioFrameCount) / sampleRate
        let totalVideoFrames = Int(duration * Double(frameRate))
        
        guard let audioBuffer = AVAudioPCMBuffer(pcmFormat: audioFormat, frameCapacity: audioFrameCount) else {
            throw ExportError.failedToReadAudioSamples
        }
        try audioFile.read(into: audioBuffer)
        
        guard let assetWriter = try? AVAssetWriter(outputURL: videoOutputURL, fileType: .mp4) else {
            throw ExportError.failedToCreateAssetWriter
        }
        
        let videoInput = AVAssetWriterInput(mediaType: .video, outputSettings: [
            AVVideoCodecKey: AVVideoCodecType.h264,
            AVVideoWidthKey: Int(videoSize.width),
            AVVideoHeightKey: Int(videoSize.height)
        ])
        videoInput.expectsMediaDataInRealTime = false
        
        let pixelBufferAdaptor = AVAssetWriterInputPixelBufferAdaptor(
            assetWriterInput: videoInput,
            sourcePixelBufferAttributes: [
                kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32ARGB,
                kCVPixelBufferWidthKey as String: Int(videoSize.width),
                kCVPixelBufferHeightKey as String: Int(videoSize.height)
            ]
        )
        
        let audioInput = AVAssetWriterInput(mediaType: .audio, outputSettings: [
            AVFormatIDKey: kAudioFormatMPEG4AAC,
            AVSampleRateKey: sampleRate,
            AVNumberOfChannelsKey: audioFormat.channelCount,
            AVEncoderBitRateKey: 64000
        ])
        audioInput.expectsMediaDataInRealTime = false
        
        if assetWriter.canAdd(videoInput) { assetWriter.add(videoInput) }
        if assetWriter.canAdd(audioInput) { assetWriter.add(audioInput) }
        
        assetWriter.startWriting()
        assetWriter.startSession(atSourceTime: .zero)
        
        let audioAsset = AVURLAsset(url: audioTempURL)
        let tracks = try await audioAsset.loadTracks(withMediaType: .audio)
        guard let audioTrack = tracks.first else { throw ExportError.trackNotFound }
        
        let assetReader = try AVAssetReader(asset: audioAsset)
        let readerOutput = AVAssetReaderTrackOutput(track: audioTrack, outputSettings: [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVLinearPCMBitDepthKey: 16,
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsFloatKey: false,
            AVLinearPCMIsNonInterleaved: false
        ])
        
        if assetReader.canAdd(readerOutput) { assetReader.add(readerOutput) }
        assetReader.startReading()
        
        let targetVideoSize = self.videoSize
        let targetFrameRate = self.frameRate
        
        struct VideoContext: @unchecked Sendable {
            let input: AVAssetWriterInput
            let adaptor: AVAssetWriterInputPixelBufferAdaptor
            let buffer: AVAudioPCMBuffer
            let scenes: [VideoScene] // ‚òÖ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Å´„ÇÇ„Ç∑„Éº„É≥„ÇíÂê´„ÇÅ„Çã
        }
        
        struct AudioContext: @unchecked Sendable {
            let input: AVAssetWriterInput
            let output: AVAssetReaderTrackOutput
        }
        
        // context‰ΩúÊàê
        let videoCtx = VideoContext(input: videoInput, adaptor: pixelBufferAdaptor, buffer: audioBuffer, scenes: scenes)
        let audioCtx = AudioContext(input: audioInput, output: readerOutput)
        
        try await withThrowingTaskGroup(of: Void.self) { group in
            
            // Task 1: Êò†ÂÉè
            group.addTask {
                await withCheckedContinuation { continuation in
                    let videoQueue = DispatchQueue(label: "videoQueue")
                    var frameIndex = 0
                    
                    videoCtx.input.requestMediaDataWhenReady(on: videoQueue) {
                        let input = videoCtx.input
                        let adaptor = videoCtx.adaptor
                        let buffer = videoCtx.buffer
                        let scenes = videoCtx.scenes
                        
                        while input.isReadyForMoreMediaData && frameIndex < totalVideoFrames {
                            let time = CMTime(value: CMTimeValue(frameIndex), timescale: targetFrameRate)
                            let seconds = Double(frameIndex) / Double(targetFrameRate)
                            
                            // ‚òÖÁèæÂú®„ÅÆÊôÇÈñì„Å´„Éû„ÉÉ„ÉÅ„Åô„Çã„Ç∑„Éº„É≥„ÇíÊé¢„Åô
                            let currentScene = scenes.first { seconds >= $0.startTime && seconds < $0.endTime }
                            let emotion = currentScene?.emotion ?? "neutral"
                            
                            let volume = getVolume(at: seconds, audioBuffer: buffer, sampleRate: sampleRate)
                            
                            // ‚òÖÊÑüÊÉÖ„ÇíÊ∏°„Åó„Å¶ÊèèÁîª
                            if let pixelBuffer = createPixelBuffer(videoSize: targetVideoSize, volume: volume, emotion: emotion) {
                                adaptor.append(pixelBuffer, withPresentationTime: time)
                            }
                            frameIndex += 1
                        }
                        
                        if frameIndex >= totalVideoFrames {
                            input.markAsFinished()
                            continuation.resume()
                        }
                    }
                }
                print("üé• Êò†ÂÉèÂÆå‰∫Ü")
            }
            
            // Task 2: Èü≥Â£∞
            group.addTask {
                await withCheckedContinuation { continuation in
                    let audioQueue = DispatchQueue(label: "audioQueue")
                    
                    audioCtx.input.requestMediaDataWhenReady(on: audioQueue) {
                        let input = audioCtx.input
                        let output = audioCtx.output
                        
                        while input.isReadyForMoreMediaData {
                            if let sampleBuffer = output.copyNextSampleBuffer() {
                                input.append(sampleBuffer)
                            } else {
                                input.markAsFinished()
                                continuation.resume()
                                break
                            }
                        }
                    }
                }
                print("üîä Èü≥Â£∞ÂÆå‰∫Ü")
            }
            
            try await group.waitForAll()
        }
        
        await assetWriter.finishWriting()
        
        if assetWriter.status == .completed {
            try? FileManager.default.removeItem(at: audioTempURL)
            print("‚úÖ ‰øùÂ≠òÊàêÂäü: \(videoOutputURL)")
            return videoOutputURL
        } else {
            throw ExportError.exportFailed(assetWriter.error?.localizedDescription ?? "Unknown")
        }
    }
}

// MARK: - Helper Functions

private func getVolume(at time: Double, audioBuffer: AVAudioPCMBuffer, sampleRate: Double) -> Float {
    guard let data = audioBuffer.floatChannelData?[0] else { return 0 }
    let index = Int(time * sampleRate)
    let window = Int(sampleRate * 0.05)
    let start = max(0, index - window/2)
    let end = min(Int(audioBuffer.frameLength)-1, index + window/2)
    guard start < end else { return 0 }
    
    var sum: Float = 0
    for i in start...end { sum += abs(data[i]) }
    return min(1.0, (sum / Float(end - start + 1)) * 5.0)
}

// ‚òÖÂºïÊï∞„Å´ emotion „ÇíËøΩÂä†
private func createPixelBuffer(videoSize: CGSize, volume: Float, emotion: String) -> CVPixelBuffer? {
    var pb: CVPixelBuffer?
    CVPixelBufferCreate(kCFAllocatorDefault, Int(videoSize.width), Int(videoSize.height), kCVPixelFormatType_32ARGB, nil, &pb)
    guard let buffer = pb else { return nil }
    
    CVPixelBufferLockBaseAddress(buffer, [])
    defer { CVPixelBufferUnlockBaseAddress(buffer, []) }
    
    let context = CGContext(
        data: CVPixelBufferGetBaseAddress(buffer),
        width: Int(videoSize.width), height: Int(videoSize.height),
        bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
        space: CGColorSpaceCreateDeviceRGB(),
        bitmapInfo: CGImageAlphaInfo.premultipliedFirst.rawValue
    )
    
    if let ctx = context {
        drawAvatar(videoSize: videoSize, context: ctx, volume: volume, emotion: emotion)
    }
    return buffer
}

// ‚òÖÊÑüÊÉÖ„Å´„Çà„ÇãÂàÜÂ≤ê„ÇíËøΩÂä†
private func drawAvatar(videoSize: CGSize, context: CGContext, volume: Float, emotion: String) {
    let w = videoSize.width, h = videoSize.height
    let cx = w/2, cy = h/2
    
    // ËÉåÊôØËâ≤: ÊÑüÊÉÖ„Å´„Çà„Å£„Å¶Â§â„Åà„Çã
    let bgColor: CGColor
    switch emotion {
    case "üòä Á¨ëÈ°î": bgColor = CGColor(red: 1.0, green: 0.9, blue: 0.9, alpha: 1) // „Éî„É≥„ÇØ
    case "üò† ÊÄí„Çä": bgColor = CGColor(red: 0.2, green: 0.0, blue: 0.0, alpha: 1) // Êöó„ÅÑËµ§
    case "üò¢ ÊÇ≤„Åó„Åø": bgColor = CGColor(red: 0.8, green: 0.8, blue: 1.0, alpha: 1) // ËñÑ„ÅÑÈùí
    default:      bgColor = CGColor(red: 1, green: 1, blue: 1, alpha: 1)       // ÁôΩ
    }
    
    context.setFillColor(bgColor)
    context.fill(CGRect(x: 0, y: 0, width: w, height: h))
    
    // È°î„ÅÆËº™ÈÉ≠
    context.setFillColor(CGColor(red: 1.0, green: 0.95, blue: 0.7, alpha: 1))
    context.fillEllipse(in: CGRect(x: cx-300, y: cy-300, width: 600, height: 600))
    
    // ÁõÆ: ÊÑüÊÉÖ„Å´„Çà„Å£„Å¶ÂΩ¢„ÇÑËâ≤„ÇíÂ§â„Åà„Çã
    context.setFillColor(CGColor(red: 0, green: 0, blue: 0, alpha: 1))
    
    if emotion == "üò† ÊÄí„Çä" {
        // Âêä„ÇäÁõÆ„Å£„ÅΩ„Åè
        context.fill(CGRect(x: cx-120, y: cy+30, width: 40, height: 20))
        context.fill(CGRect(x: cx+80, y: cy+30, width: 40, height: 20))
    } else if emotion == "üòä Á¨ëÈ°î" {
        // „Ç¢„Éº„ÉÅÁä∂„ÅÆÁõÆÔºàÁ∞°ÊòìÁöÑ„Å´Á¥∞„ÅèÔºâ
        context.fillEllipse(in: CGRect(x: cx-120, y: cy+40, width: 40, height: 20))
        context.fillEllipse(in: CGRect(x: cx+80, y: cy+40, width: 40, height: 20))
    } else {
        // ÊôÆÈÄö„ÅÆÁõÆ
        context.fillEllipse(in: CGRect(x: cx-120, y: cy+30, width: 40, height: 60))
        context.fillEllipse(in: CGRect(x: cx+80, y: cy+30, width: 40, height: 60))
    }
    
    // Âè£ („Éë„ÇØ„Éë„ÇØ)
    let mH = 10 + (70 * CGFloat(volume))
    context.setFillColor(CGColor(red: 0.9, green: 0.2, blue: 0.2, alpha: 1))
    context.fillEllipse(in: CGRect(x: cx-50, y: cy-100-mH/2, width: 100, height: mH))
}
