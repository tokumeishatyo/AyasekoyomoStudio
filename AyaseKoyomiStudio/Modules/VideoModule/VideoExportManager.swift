import Foundation
@preconcurrency import AVFoundation
import CoreImage
import AppKit

// ‚òÖ‚òÖ‚òÖ „ÇØ„É©„ÇπÂÖ®‰Ωì„Çí MainActor „Å´„Åó„Å¶UI„Çπ„É¨„ÉÉ„Éâ„ÅßÁÆ°ÁêÜ ‚òÖ‚òÖ‚òÖ
@MainActor
final class VideoExportManager: NSObject {
    
    static let shared = VideoExportManager()
    private override init() { super.init() }
    
    private let videoSize = CGSize(width: 1080, height: 1080)
    private let frameRate: Int32 = 30
    
    enum ExportError: LocalizedError {
        case failedToCreateAssetWriter, failedToCreateAudioFile, failedToReadAudioSamples, trackNotFound
        case exportFailed(String)
        
        var errorDescription: String? {
            switch self {
            case .failedToCreateAssetWriter: return "AssetWriter‰ΩúÊàêÂ§±Êïó"
            case .failedToCreateAudioFile: return "Èü≥Â£∞„Éï„Ç°„Ç§„É´Ë™≠ËæºÂ§±Êïó"
            case .failedToReadAudioSamples: return "Èü≥Â£∞„Çµ„É≥„Éó„É´Ë™≠ËæºÂ§±Êïó"
            case .trackNotFound: return "„Éà„É©„ÉÉ„ÇØ‰∏çÊòé"
            case .exportFailed(let msg): return "Êõ∏„ÅçÂá∫„ÅóÂ§±Êïó: \(msg)"
            }
        }
    }
    
    // ‚òÖ‚òÖ‚òÖ nonisolated „Åß„É°„Ç§„É≥„Çπ„É¨„ÉÉ„Éâ„Åã„ÇâÂàá„ÇäÈõ¢„Åó„Å¶ÂÆüË°å ‚òÖ‚òÖ‚òÖ
    nonisolated func exportVideo(audioData: Data) async throws -> URL {
        print("üé• Export: ÈñãÂßã")
        
        let tempDir = FileManager.default.temporaryDirectory
        let uuid = UUID().uuidString
        let audioTempURL = tempDir.appendingPathComponent("\(uuid).mp3")
        let videoOutputURL = tempDir.appendingPathComponent("\(uuid).mp4")
        
        try? FileManager.default.removeItem(at: audioTempURL)
        try? FileManager.default.removeItem(at: videoOutputURL)
        try audioData.write(to: audioTempURL)
        
        let audioFile = try AVAudioFile(forReading: audioTempURL)
        let audioFormat = audioFile.processingFormat
        let audioFrameCount = AVAudioFrameCount(audioFile.length)
        let sampleRate = audioFormat.sampleRate
        let duration = Double(audioFrameCount) / sampleRate
        let totalVideoFrames = Int(duration * Double(frameRate))
        
        guard let audioBuffer = AVAudioPCMBuffer(pcmFormat: audioFormat, frameCapacity: audioFrameCount) else {
            throw ExportError.failedToReadAudioSamples
        }
        try audioFile.read(into: audioBuffer)
        
        guard let assetWriter = try? AVAssetWriter(outputURL: videoOutputURL, fileType: .mp4) else {
            throw ExportError.failedToCreateAssetWriter
        }
        
        let videoInput = AVAssetWriterInput(mediaType: .video, outputSettings: [
            AVVideoCodecKey: AVVideoCodecType.h264,
            AVVideoWidthKey: Int(videoSize.width),
            AVVideoHeightKey: Int(videoSize.height)
        ])
        videoInput.expectsMediaDataInRealTime = false
        
        let pixelBufferAdaptor = AVAssetWriterInputPixelBufferAdaptor(
            assetWriterInput: videoInput,
            sourcePixelBufferAttributes: [
                kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32ARGB,
                kCVPixelBufferWidthKey as String: Int(videoSize.width),
                kCVPixelBufferHeightKey as String: Int(videoSize.height)
            ]
        )
        
        let audioInput = AVAssetWriterInput(mediaType: .audio, outputSettings: [
            AVFormatIDKey: kAudioFormatMPEG4AAC,
            AVSampleRateKey: sampleRate,
            AVNumberOfChannelsKey: audioFormat.channelCount,
            AVEncoderBitRateKey: 128000
        ])
        audioInput.expectsMediaDataInRealTime = false
        
        if assetWriter.canAdd(videoInput) { assetWriter.add(videoInput) }
        if assetWriter.canAdd(audioInput) { assetWriter.add(audioInput) }
        
        assetWriter.startWriting()
        assetWriter.startSession(atSourceTime: .zero)
        
        let audioAsset = AVURLAsset(url: audioTempURL)
        let tracks = try await audioAsset.loadTracks(withMediaType: .audio)
        guard let audioTrack = tracks.first else { throw ExportError.trackNotFound }
        
        let assetReader = try AVAssetReader(asset: audioAsset)
        let readerOutput = AVAssetReaderTrackOutput(track: audioTrack, outputSettings: [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVLinearPCMBitDepthKey: 16,
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsFloatKey: false,
            AVLinearPCMIsNonInterleaved: false
        ])
        
        if assetReader.canAdd(readerOutput) { assetReader.add(readerOutput) }
        assetReader.startReading()
        
        let targetVideoSize = self.videoSize
        let targetFrameRate = self.frameRate
        
        // ‚òÖ‚òÖ‚òÖ „Ç≥„É≥„Éë„Ç§„É©„ÉÅ„Çß„ÉÉ„ÇØ„ÇíÂõûÈÅø„Åô„Çã„Åü„ÇÅ„ÅÆÂÆâÂÖ®„Å™ÁÆ± ‚òÖ‚òÖ‚òÖ
        struct VideoContext: @unchecked Sendable {
            let input: AVAssetWriterInput
            let adaptor: AVAssetWriterInputPixelBufferAdaptor
            let buffer: AVAudioPCMBuffer
        }
        
        struct AudioContext: @unchecked Sendable {
            let input: AVAssetWriterInput
            let output: AVAssetReaderTrackOutput
        }
        
        let videoCtx = VideoContext(input: videoInput, adaptor: pixelBufferAdaptor, buffer: audioBuffer)
        let audioCtx = AudioContext(input: audioInput, output: readerOutput)
        
        try await withThrowingTaskGroup(of: Void.self) { group in
            
            // Task 1: Êò†ÂÉè
            group.addTask {
                await withCheckedContinuation { continuation in
                    let videoQueue = DispatchQueue(label: "videoQueue")
                    var frameIndex = 0
                    
                    // ‚òÖ‚òÖ‚òÖ ‰øÆÊ≠£„Éù„Ç§„É≥„Éà: „Åì„Åì„Åß videoCtx (ÁÆ±) „Çí‰Ωø„Å£„Å¶Âëº„Å≥Âá∫„Åô ‚òÖ‚òÖ‚òÖ
                    // ‰∏≠Ë∫´ (input) „Çí„É≠„Éº„Ç´„É´Â§âÊï∞„Å´„Åó„Å¶„Åã„ÇâÂëº„Å≥Âá∫„Åô„Å®„ÄÅ„Åù„ÅÆÂ§âÊï∞„Åå„Ç≠„É£„Éó„ÉÅ„É£„Åï„Çå„Å¶Warning„Å´„Å™„Çã„Åü„ÇÅ
                    videoCtx.input.requestMediaDataWhenReady(on: videoQueue) {
                        
                        // ‚òÖ‚òÖ‚òÖ „ÇØ„É≠„Éº„Ç∏„É£„ÅÆ„Äå‰∏≠„Äç„ÅßÁÆ±„ÇíÈñã„Åë„Çã ‚òÖ‚òÖ‚òÖ
                        // „Åì„Åì„Å™„ÇâÂÆüË°å„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÂÜÖ„Å™„ÅÆ„ÅßÂÆâÂÖ®„Å´„Ç¢„ÇØ„Çª„Çπ„Åß„Åç„Çã
                        let input = videoCtx.input
                        let adaptor = videoCtx.adaptor
                        let buffer = videoCtx.buffer
                        
                        while input.isReadyForMoreMediaData && frameIndex < totalVideoFrames {
                            let time = CMTime(value: CMTimeValue(frameIndex), timescale: targetFrameRate)
                            let seconds = Double(frameIndex) / Double(targetFrameRate)
                            
                            let volume = getVolume(at: seconds, audioBuffer: buffer, sampleRate: sampleRate)
                            if let pixelBuffer = createPixelBuffer(videoSize: targetVideoSize, volume: volume) {
                                adaptor.append(pixelBuffer, withPresentationTime: time)
                            }
                            frameIndex += 1
                        }
                        
                        if frameIndex >= totalVideoFrames {
                            input.markAsFinished()
                            continuation.resume()
                        }
                    }
                }
                print("üé• Êò†ÂÉèÂÆå‰∫Ü")
            }
            
            // Task 2: Èü≥Â£∞
            group.addTask {
                await withCheckedContinuation { continuation in
                    let audioQueue = DispatchQueue(label: "audioQueue")
                    
                    // ‚òÖ‚òÖ‚òÖ ‰øÆÊ≠£„Éù„Ç§„É≥„Éà: Èü≥Â£∞ÂÅ¥„ÇÇÂêåÊßò„Å´ÁÆ± (audioCtx) „Çí‰Ωø„ÅÜ ‚òÖ‚òÖ‚òÖ
                    audioCtx.input.requestMediaDataWhenReady(on: audioQueue) {
                        let input = audioCtx.input
                        let output = audioCtx.output
                        
                        while input.isReadyForMoreMediaData {
                            if let sampleBuffer = output.copyNextSampleBuffer() {
                                input.append(sampleBuffer)
                            } else {
                                input.markAsFinished()
                                continuation.resume()
                                break
                            }
                        }
                    }
                }
                print("üîä Èü≥Â£∞ÂÆå‰∫Ü")
            }
            
            try await group.waitForAll()
        }
        
        await assetWriter.finishWriting()
        
        if assetWriter.status == .completed {
            try? FileManager.default.removeItem(at: audioTempURL)
            print("‚úÖ ‰øùÂ≠òÊàêÂäü: \(videoOutputURL)")
            return videoOutputURL
        } else {
            throw ExportError.exportFailed(assetWriter.error?.localizedDescription ?? "Unknown")
        }
    }
}

// MARK: - Helper Functions („ÇØ„É©„ÇπÂ§ñ)

private func getVolume(at time: Double, audioBuffer: AVAudioPCMBuffer, sampleRate: Double) -> Float {
    guard let data = audioBuffer.floatChannelData?[0] else { return 0 }
    let index = Int(time * sampleRate)
    let window = Int(sampleRate * 0.05)
    let start = max(0, index - window/2)
    let end = min(Int(audioBuffer.frameLength)-1, index + window/2)
    guard start < end else { return 0 }
    
    var sum: Float = 0
    for i in start...end { sum += abs(data[i]) }
    return min(1.0, (sum / Float(end - start + 1)) * 5.0)
}

private func createPixelBuffer(videoSize: CGSize, volume: Float) -> CVPixelBuffer? {
    var pb: CVPixelBuffer?
    CVPixelBufferCreate(kCFAllocatorDefault, Int(videoSize.width), Int(videoSize.height), kCVPixelFormatType_32ARGB, nil, &pb)
    guard let buffer = pb else { return nil }
    
    CVPixelBufferLockBaseAddress(buffer, [])
    defer { CVPixelBufferUnlockBaseAddress(buffer, []) }
    
    let context = CGContext(
        data: CVPixelBufferGetBaseAddress(buffer),
        width: Int(videoSize.width), height: Int(videoSize.height),
        bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
        space: CGColorSpaceCreateDeviceRGB(),
        bitmapInfo: CGImageAlphaInfo.premultipliedFirst.rawValue
    )
    
    if let ctx = context {
        drawAvatar(videoSize: videoSize, context: ctx, volume: volume)
    }
    return buffer
}

private func drawAvatar(videoSize: CGSize, context: CGContext, volume: Float) {
    let w = videoSize.width, h = videoSize.height
    let cx = w/2, cy = h/2
    
    context.setFillColor(CGColor(red: 1, green: 1, blue: 1, alpha: 1))
    context.fill(CGRect(x: 0, y: 0, width: w, height: h))
    
    context.setFillColor(CGColor(red: 1.0, green: 0.95, blue: 0.7, alpha: 1))
    context.fillEllipse(in: CGRect(x: cx-300, y: cy-300, width: 600, height: 600))
    
    context.setFillColor(CGColor(red: 0, green: 0, blue: 0, alpha: 1))
    context.fillEllipse(in: CGRect(x: cx-120, y: cy+30, width: 40, height: 60))
    context.fillEllipse(in: CGRect(x: cx+80, y: cy+30, width: 40, height: 60))
    
    let mH = 10 + (70 * CGFloat(volume))
    context.setFillColor(CGColor(red: 0.9, green: 0.2, blue: 0.2, alpha: 1))
    context.fillEllipse(in: CGRect(x: cx-50, y: cy-100-mH/2, width: 100, height: mH))
}
