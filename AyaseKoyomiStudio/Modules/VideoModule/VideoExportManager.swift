import Foundation
@preconcurrency import AVFoundation
import CoreImage
import AppKit

// â˜…â˜…â˜… æ¼”å‡ºæŒ‡ç¤ºæ›¸ (ã©ã®æ™‚é–“ã«ã©ã‚“ãªé¡”ã‚’ã™ã‚‹ã‹) â˜…â˜…â˜…
struct VideoScene: Sendable {
    let startTime: Double
    let endTime: Double
    let emotion: String // "happy", "angry", "sad", "neutral"
    let backgroundURL: URL? // â˜…èƒŒæ™¯ç”»åƒ (Lv.2)
}

@MainActor
final class VideoExportManager: NSObject {
    
    static let shared = VideoExportManager()
    private override init() { super.init() }
    
    private let videoSize = CGSize(width: 1920, height: 1080)
    private let frameRate: Int32 = 30
    
    enum ExportError: LocalizedError {
        case failedToCreateAssetWriter, failedToCreateAudioFile, failedToReadAudioSamples, trackNotFound
        case exportFailed(String)
        
        var errorDescription: String? {
            switch self {
            case .failedToCreateAssetWriter: return "AssetWriterä½œæˆå¤±æ•—"
            case .failedToCreateAudioFile: return "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼å¤±æ•—"
            case .failedToReadAudioSamples: return "éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«èª­è¾¼å¤±æ•—"
            case .trackNotFound: return "ãƒˆãƒ©ãƒƒã‚¯ä¸æ˜"
            case .exportFailed(let msg): return "æ›¸ãå‡ºã—å¤±æ•—: \(msg)"
            }
        }
    }
    
    // â˜…â˜…â˜… å¤‰æ›´ç‚¹: avatarImageURL è¿½åŠ  â˜…â˜…â˜…
    nonisolated func exportVideo(audioData: Data, scenes: [VideoScene], resolution: VideoResolution, avatarImageURL: URL?) async throws -> URL {
        let videoSize = resolution.size
        print("ğŸ¥ Export: é–‹å§‹ (ã‚·ãƒ¼ãƒ³æ•°: \(scenes.count), è§£åƒåº¦: \(resolution.name))")
        
        let tempDir = FileManager.default.temporaryDirectory
        let uuid = UUID().uuidString
        let audioTempURL = tempDir.appendingPathComponent("\(uuid).mp3")
        let videoOutputURL = tempDir.appendingPathComponent("\(uuid).mp4")
        
        try? FileManager.default.removeItem(at: audioTempURL)
        try? FileManager.default.removeItem(at: videoOutputURL)
        try audioData.write(to: audioTempURL)
        
        let audioFile = try AVAudioFile(forReading: audioTempURL)
        let audioFormat = audioFile.processingFormat
        let audioFrameCount = AVAudioFrameCount(audioFile.length)
        let sampleRate = audioFormat.sampleRate
        let duration = Double(audioFrameCount) / sampleRate
        let totalVideoFrames = Int(duration * Double(frameRate))
        
        guard let audioBuffer = AVAudioPCMBuffer(pcmFormat: audioFormat, frameCapacity: audioFrameCount) else {
            throw ExportError.failedToReadAudioSamples
        }
        try audioFile.read(into: audioBuffer)
        
        guard let assetWriter = try? AVAssetWriter(outputURL: videoOutputURL, fileType: .mp4) else {
            throw ExportError.failedToCreateAssetWriter
        }
        
        let videoInput = AVAssetWriterInput(mediaType: .video, outputSettings: [
            AVVideoCodecKey: AVVideoCodecType.h264,
            AVVideoWidthKey: Int(videoSize.width),
            AVVideoHeightKey: Int(videoSize.height)
        ])
        videoInput.expectsMediaDataInRealTime = false
        
        let pixelBufferAdaptor = AVAssetWriterInputPixelBufferAdaptor(
            assetWriterInput: videoInput,
            sourcePixelBufferAttributes: [
                kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32ARGB,
                kCVPixelBufferWidthKey as String: Int(videoSize.width),
                kCVPixelBufferHeightKey as String: Int(videoSize.height)
            ]
        )
        
        let audioInput = AVAssetWriterInput(mediaType: .audio, outputSettings: [
            AVFormatIDKey: kAudioFormatMPEG4AAC,
            AVSampleRateKey: sampleRate,
            AVNumberOfChannelsKey: audioFormat.channelCount,
            AVEncoderBitRateKey: 64000
        ])
        audioInput.expectsMediaDataInRealTime = false
        
        if assetWriter.canAdd(videoInput) { assetWriter.add(videoInput) }
        if assetWriter.canAdd(audioInput) { assetWriter.add(audioInput) }
        
        assetWriter.startWriting()
        assetWriter.startSession(atSourceTime: .zero)
        
        let audioAsset = AVURLAsset(url: audioTempURL)
        let tracks = try await audioAsset.loadTracks(withMediaType: .audio)
        guard let audioTrack = tracks.first else { throw ExportError.trackNotFound }
        
        let assetReader = try AVAssetReader(asset: audioAsset)
        let readerOutput = AVAssetReaderTrackOutput(track: audioTrack, outputSettings: [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVLinearPCMBitDepthKey: 16,
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsFloatKey: false,
            AVLinearPCMIsNonInterleaved: false
        ])
        
        if assetReader.canAdd(readerOutput) { assetReader.add(readerOutput) }
        assetReader.startReading()
        
        let targetVideoSize = videoSize // ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã‚’ä½¿ç”¨
        let targetFrameRate = self.frameRate
        
        struct VideoContext: @unchecked Sendable {
            let input: AVAssetWriterInput
            let adaptor: AVAssetWriterInputPixelBufferAdaptor
            let buffer: AVAudioPCMBuffer
            let scenes: [VideoScene]
            let avatarImageURL: URL? // â˜…è¿½åŠ 
        }
        
        struct AudioContext: @unchecked Sendable {
            let input: AVAssetWriterInput
            let output: AVAssetReaderTrackOutput
        }
        
        // contextä½œæˆ
        let videoCtx = VideoContext(input: videoInput, adaptor: pixelBufferAdaptor, buffer: audioBuffer, scenes: scenes, avatarImageURL: avatarImageURL)
        let audioCtx = AudioContext(input: audioInput, output: readerOutput)
        
        try await withThrowingTaskGroup(of: Void.self) { group in
            
            // Task 1: æ˜ åƒ
            group.addTask {
                await withCheckedContinuation { continuation in
                    let videoQueue = DispatchQueue(label: "videoQueue")
                    var frameIndex = 0
                    
                    videoCtx.input.requestMediaDataWhenReady(on: videoQueue) {
                        let input = videoCtx.input
                        let adaptor = videoCtx.adaptor
                        let buffer = videoCtx.buffer
                        let scenes = videoCtx.scenes
                        let avatarURL = videoCtx.avatarImageURL
                        
                        while input.isReadyForMoreMediaData && frameIndex < totalVideoFrames {
                            let time = CMTime(value: CMTimeValue(frameIndex), timescale: targetFrameRate)
                            let seconds = Double(frameIndex) / Double(targetFrameRate)
                            
                            // â˜…ç¾åœ¨ã®æ™‚é–“ã«ãƒãƒƒãƒã™ã‚‹ã‚·ãƒ¼ãƒ³ã‚’æ¢ã™
                            let currentScene = scenes.first { seconds >= $0.startTime && seconds < $0.endTime }
                            let emotion = currentScene?.emotion ?? "neutral"
                            let bgURL = currentScene?.backgroundURL
                            
                            let volume = getVolume(at: seconds, audioBuffer: buffer, sampleRate: sampleRate)
                            
                            // â˜…æ„Ÿæƒ…ã‚’æ¸¡ã—ã¦æç”»
                            if let pixelBuffer = createPixelBuffer(videoSize: targetVideoSize, volume: volume, emotion: emotion, backgroundURL: bgURL, avatarImageURL: avatarURL) {
                                adaptor.append(pixelBuffer, withPresentationTime: time)
                            }
                            frameIndex += 1
                        }
                        
                        if frameIndex >= totalVideoFrames {
                            input.markAsFinished()
                            continuation.resume()
                        }
                    }
                }
                print("ğŸ¥ æ˜ åƒå®Œäº†")
            }
            
            // Task 2: éŸ³å£°
            group.addTask {
                await withCheckedContinuation { continuation in
                    let audioQueue = DispatchQueue(label: "audioQueue")
                    
                    audioCtx.input.requestMediaDataWhenReady(on: audioQueue) {
                        let input = audioCtx.input
                        let output = audioCtx.output
                        
                        while input.isReadyForMoreMediaData {
                            if let sampleBuffer = output.copyNextSampleBuffer() {
                                input.append(sampleBuffer)
                            } else {
                                input.markAsFinished()
                                continuation.resume()
                                break
                            }
                        }
                    }
                }
                print("ğŸ”Š éŸ³å£°å®Œäº†")
            }
            
            try await group.waitForAll()
        }
        
        await assetWriter.finishWriting()
        
        if assetWriter.status == .completed {
            try? FileManager.default.removeItem(at: audioTempURL)
            print("âœ… ä¿å­˜æˆåŠŸ: \(videoOutputURL)")
            return videoOutputURL
        } else {
            throw ExportError.exportFailed(assetWriter.error?.localizedDescription ?? "Unknown")
        }
    }
}

// MARK: - Helper Functions

private func getVolume(at time: Double, audioBuffer: AVAudioPCMBuffer, sampleRate: Double) -> Float {
    guard let data = audioBuffer.floatChannelData?[0] else { return 0 }
    let index = Int(time * sampleRate)
    let window = Int(sampleRate * 0.05)
    let start = max(0, index - window/2)
    let end = min(Int(audioBuffer.frameLength)-1, index + window/2)
    guard start < end else { return 0 }
    
    var sum: Float = 0
    for i in start...end { sum += abs(data[i]) }
    return min(1.0, (sum / Float(end - start + 1)) * 5.0)
}

// â˜…å¼•æ•°ã« backgroundURL ã‚’è¿½åŠ 
private func createPixelBuffer(videoSize: CGSize, volume: Float, emotion: String, backgroundURL: URL?, avatarImageURL: URL?) -> CVPixelBuffer? {
    var pb: CVPixelBuffer?
    CVPixelBufferCreate(kCFAllocatorDefault, Int(videoSize.width), Int(videoSize.height), kCVPixelFormatType_32ARGB, nil, &pb)
    guard let buffer = pb else { return nil }
    
    CVPixelBufferLockBaseAddress(buffer, [])
    defer { CVPixelBufferUnlockBaseAddress(buffer, []) }
    
    let context = CGContext(
        data: CVPixelBufferGetBaseAddress(buffer),
        width: Int(videoSize.width), height: Int(videoSize.height),
        bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
        space: CGColorSpaceCreateDeviceRGB(),
        bitmapInfo: CGImageAlphaInfo.premultipliedFirst.rawValue
    )
    
    if let ctx = context {
        drawAvatar(videoSize: videoSize, context: ctx, volume: volume, emotion: emotion, backgroundURL: backgroundURL, avatarImageURL: avatarImageURL)
    }
    return buffer
}

// â˜…èƒŒæ™¯ç”»åƒå¯¾å¿œ
private func drawAvatar(videoSize: CGSize, context: CGContext, volume: Float, emotion: String, backgroundURL: URL?, avatarImageURL: URL?) {
    let w = videoSize.width, h = videoSize.height
    let cx = w/2, cy = h/2
    
    // 1. èƒŒæ™¯æç”»
    var backgroundDrawn = false
    
    if let bgURL = backgroundURL,
       let nsImage = NSImage(contentsOf: bgURL),
       let list = nsImage.cgImage(forProposedRect: nil, context: nil, hints: nil) {
        
        // ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ç¶­æŒã—ã¤ã¤ä¸­å¤®é…ç½® (Fill)
        context.saveGState()
        // ä¸Šä¸‹åè»¢å¯¾ç­– (CoreGraphicsã¯åº§æ¨™ç³»ãŒå·¦ä¸‹åŸºæº–ã€ç”»åƒã¯å·¦ä¸ŠåŸºæº–ã®å ´åˆãŒã‚ã‚‹ãŒã€CGContextã®CTMã«ã‚ˆã‚‹)
        // ã“ã“ã§ã¯å˜ç´”ã«æç”»ã—ã¦ã¿ã‚‹ã€‚å¿…è¦ãªã‚‰CTMèª¿æ•´ã€‚
        
        context.draw(list, in: CGRect(x: 0, y: 0, width: w, height: h))
        context.restoreGState()
        backgroundDrawn = true
    }
    
    if !backgroundDrawn {
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ„Ÿæƒ…ã«ã‚ˆã‚‹è‰²å¤‰ãˆ
        let bgColor: CGColor
        switch emotion {
        case "ğŸ˜Š ç¬‘é¡”": bgColor = CGColor(red: 1.0, green: 0.9, blue: 0.9, alpha: 1) // ãƒ”ãƒ³ã‚¯
        case "ğŸ˜  æ€’ã‚Š": bgColor = CGColor(red: 0.2, green: 0.0, blue: 0.0, alpha: 1) // æš—ã„èµ¤
        case "ğŸ˜¢ æ‚²ã—ã¿": bgColor = CGColor(red: 0.8, green: 0.8, blue: 1.0, alpha: 1) // è–„ã„é’
        default:      bgColor = CGColor(red: 1, green: 1, blue: 1, alpha: 1)       // ç™½
        }
        
        context.setFillColor(bgColor)
        context.fill(CGRect(x: 0, y: 0, width: w, height: h))
    }
    
    // 2. ã‚¢ãƒã‚¿ãƒ¼æç”»
    
    // â˜…ç”»åƒãŒã‚ã‚‹å ´åˆ
    if let avatarURL = avatarImageURL,
       let nsImage = NSImage(contentsOf: avatarURL),
       let cgImage = nsImage.cgImage(forProposedRect: nil, context: nil, hints: nil) {
        
        context.saveGState()
        // å††å½¢ã‚¯ãƒªãƒƒãƒ—
        context.addEllipse(in: CGRect(x: cx-300, y: cy-300, width: 600, height: 600))
        context.clip()
        
        // ç”»åƒæç”»
        context.draw(cgImage, in: CGRect(x: cx-300, y: cy-300, width: 600, height: 600))
        context.restoreGState()
        
    } else {
        // é¡”ã®è¼ªéƒ­ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
        context.setFillColor(CGColor(red: 1.0, green: 0.95, blue: 0.7, alpha: 1))
        context.fillEllipse(in: CGRect(x: cx-300, y: cy-300, width: 600, height: 600))
    }
    
    // ç›®: æ„Ÿæƒ…ã«ã‚ˆã£ã¦å½¢ã‚„è‰²ã‚’å¤‰ãˆã‚‹
    context.setFillColor(CGColor(red: 0, green: 0, blue: 0, alpha: 1))
    
    if emotion == "ğŸ˜  æ€’ã‚Š" {
        // åŠã‚Šç›®ã£ã½ã
        context.fill(CGRect(x: cx-120, y: cy+30, width: 40, height: 20))
        context.fill(CGRect(x: cx+80, y: cy+30, width: 40, height: 20))
    } else if emotion == "ğŸ˜Š ç¬‘é¡”" {
        // ã‚¢ãƒ¼ãƒçŠ¶ã®ç›®ï¼ˆç°¡æ˜“çš„ã«ç´°ãï¼‰
        context.fillEllipse(in: CGRect(x: cx-120, y: cy+40, width: 40, height: 20))
        context.fillEllipse(in: CGRect(x: cx+80, y: cy+40, width: 40, height: 20))
    } else {
        // æ™®é€šã®ç›®
        context.fillEllipse(in: CGRect(x: cx-120, y: cy+30, width: 40, height: 60))
        context.fillEllipse(in: CGRect(x: cx+80, y: cy+30, width: 40, height: 60))
    }
    
    // å£ (ãƒ‘ã‚¯ãƒ‘ã‚¯)
    let mH = 10 + (70 * CGFloat(volume))
    context.setFillColor(CGColor(red: 0.9, green: 0.2, blue: 0.2, alpha: 1))
    context.fillEllipse(in: CGRect(x: cx-50, y: cy-100-mH/2, width: 100, height: mH))
}
