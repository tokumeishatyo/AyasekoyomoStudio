# AIアバター動画生成スタジオ 機能仕様書

**バージョン:** 0.2 (Revised)
**対象OS:** macOS 14.0+ (Sonoma以降推奨)
**開発言語:** Swift (SwiftUI)

---

## 目次

1. [はじめに](https://www.google.com/search?q=%231-%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB)
2. [システムアーキテクチャ（モジュール設計）](https://www.google.com/search?q=%232-%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3%E3%83%A2%E3%82%B8%E3%83%A5%E3%83%BC%E3%83%AB%E8%A8%AD%E8%A8%88)
3. [開発フェーズ定義 (Lv.1 - Lv.4)](https://www.google.com/search?q=%233-%E9%96%8B%E7%99%BA%E3%83%95%E3%82%A7%E3%83%BC%E3%82%BA%E5%AE%9A%E7%BE%A9-lv1---lv4)
4. [モジュール詳細仕様・API連携](https://www.google.com/search?q=%234-%E3%83%A2%E3%82%B8%E3%83%A5%E3%83%BC%E3%83%AB%E8%A9%B3%E7%B4%B0%E4%BB%95%E6%A7%98api%E9%80%A3%E6%90%BA)
5. [データ構造](https://www.google.com/search?q=%235-%E3%83%87%E3%83%BC%E3%82%BF%E6%A7%8B%E9%80%A0)
6. [非機能要件](https://www.google.com/search?q=%236-%E9%9D%9E%E6%A9%9F%E8%83%BD%E8%A6%81%E4%BB%B6)

---

## 1. はじめに

### 1.1 プロジェクト概要

本アプリケーションは、**Google Gemini API** およびTTS技術を活用し、キャラクター（彩瀬こよみ等）が台本を読み上げ、表情豊かに解説する動画を自動生成するmacOSネイティブアプリケーションです。

### 1.2 デザイン哲学：徹底したモジュール化

WindowsにおけるDLL構成のように、機能ごとに明確に責務を分離します。メインアプリ（UI層）は「指揮者」に徹し、実際の処理（画像生成、音声解析、動画書き出し）は独立したモジュール（Worker）が行います。

### 1.3 使用技術スタック

| レイヤー | 技術 | 備考 |
| --- | --- | --- |
| **UI FW** | SwiftUI | macOSネイティブUI |
| **言語** | Swift 5.9+ |  |
| **音声処理** | AVFoundation | 再生、波形解析、リップシンク判定 |
| **動画出力** | AVAssetWriter | オフラインレンダリング |
| **AI通信** | **Google Generative AI SDK** | **Gemini API (Speech / Image / Text)** |

---

## 2. システムアーキテクチャ（モジュール設計）

各モジュールは相互依存を避け、`Protocol`（インターフェース）を通じてのみ通信します。API通信は `AIInfra` モジュールが一元管理します。

### 2.1 全体構成図

```mermaid
graph TD
    User[ユーザー] --> MainApp[Main App (UI & Orchestrator)]
    
    subgraph "Logic Modules (Library Candidates)"
        MainApp --> TimelineMod[Timeline Module]
        MainApp --> AvatarMod[Avatar Visual Module]
        MainApp --> VoiceMod[Voice & Audio Module]
        MainApp --> SceneMod[Scene & Background Module]
        MainApp --> ExportMod[Video Export Module]
    end
    
    subgraph "Infrastructure (Gateway)"
        AvatarMod --> AIInfra[AIInfra (API Client)]
        VoiceMod --> AIInfra
        SceneMod --> AIInfra
        TimelineMod --> AIInfra
        AIInfra == "HTTPS / API Key" ==> GoogleCloud[Google Gemini API]
    end

```

### 2.2 モジュール一覧

| モジュール名 | 役割 | 入力 | 出力 |
| --- | --- | --- | --- |
| **AIInfra** | Google APIとの通信ラッパー | プロンプト/テキスト | JSON/画像データ/音声データ |
| **VoiceModule** | 音声再生、波形解析(リップシンク) | テキスト/音声ファイル | 現在の音量(dB)/再生状態 |
| **AvatarModule** | 表情管理、口パク描画状態の決定 | 現在の音量/感情タグ | 表示すべき画像ID |
| **SceneModule** | 背景・図解の管理と生成 | 背景プロンプト | 背景画像/配置情報 |
| **TimelineModule** | シナリオ進行管理(指揮者) | スクリプトデータ | 現在のセリフ/再生位置 |
| **ExportModule** | 動画ファイル書き出し | 画像・音声アセット・構成情報 | .mp4ファイル |

---

## 3. 開発フェーズ定義 (Lv.1 - Lv.4)

アジャイル開発的に、以下の段階を経て機能を実装します。

### Lv.1: ベーシック・トーキング (MVP)

**目標:** 「テキストを入れると、キャラが喋って動く」体験の確立。

* **機能:**
* テキスト入力エリアの実装。
* Gemini/TTSによる音声生成。
* 音声再生とリアルタイム音量解析。
* 音量閾値による画像切り替え（口パク）。
* 表情のランダム切り替え（まばたき等）。



### Lv.2: シーン・コンポーザー

**目標:** 画面構成要素の拡充。

* **機能:**
* **SceneModule** の実装。
* 背景画像のインポートおよびGeminiによる生成。
* キャラクターと背景のレイヤー合成（ZStack制御）。
* キャラクターの立ち位置調整（左・中央・右）。



### Lv.3: タイムライン・ディレクター

**目標:** 「動画」としての構成力の獲得。

* **機能:**
* **TimelineModule** の実装。
* 複数の「セリフブロック」をリスト管理。
* ブロックごとの演出設定（このセリフでは笑顔、背景はオフィス、など）。
* 連続再生機能。



### Lv.4: ビデオ・レンダラー

**目標:** 成果物の永続化。

* **機能:**
* **ExportModule** の実装。
* リアルタイムプレビューとは別に、フレーム単位で画像を合成し、音声と結合して `.mp4` を書き出す。
* 1080p / 30fps または 60fps 出力。

---

## 4. モジュール詳細仕様・API連携

ご指摘ありがとうございます。**問題があります。**

ご懸念の通り、バージョン2を作成する際、「API連携」に焦点を当てるあまり、バージョン1にあった**「具体的な実装ロジック（ステートマシンやプロトコル定義など）」**が省略されてしまいました。

実装（プログラミング）を行う段階では、**「どう通信するか（Ver.2の内容）」**と**「内部でどう動くか（Ver.1の内容）」**の両方の情報が不可欠です。

この2つを統合した**完全版（Version 3）の「4. モジュール詳細仕様」**を作成しました。これを正としてください。

---

# 4. モジュール詳細仕様 (統合・完全版)

内部ロジック（Swift実装）と外部通信（Gemini API）の両方を記載しています。

### 4.1 VoiceModule (音声・リップシンク)

**責務:** 音声データの生成・管理・再生、およびリップシンク用の波形解析。

* **使用するAPI:**
* **Google Gemini API (Speech Generation)** / `gemini-1.5-flash` 等


* **インターフェース (Protocol):**
```swift
protocol VoiceEngineDelegate {
    func audioDidFinishPlaying()
    func audioAmplitudeDidUpdate(level: Float) // 0.0〜1.0 (正規化された音量)
}

```


* **内部ロジック (Implementation):**
* **再生:** `AVAudioPlayer` または `AVAudioEngine` を使用。
* **解析:** タイマー (`Timer` / `CADisplayLink`) で再生中の `averagePower` を監視し、デシベル値を 0.0〜1.0 のリニアな値に変換して Delegate に通知する。


* **API通信フロー:**
1. UIからテキストを受信。
2. ローカルキャッシュ（ハッシュ値で管理）を確認。
3. なければ API (`POST /v1beta/models/...:generateContent`) へリクエスト。
4. 返却された音声バイナリを保存し、再生準備完了を通知。



### 4.2 AvatarModule (キャラクター表現)

**責務:** キャラクターの「状態」管理と、描画すべき画像IDの提供。

* **使用するAPI:**
* **Google Gemini API (Image Generation)** / `imagen-3.0-generate-001` 等


* **内部状態管理 (State Machine):**
* **Eyes:** `Open` (通常), `Closed` (瞬き/眠り), `Smile` (笑顔)
* **Mouth:** `Closed` (無音), `Open` (小), `Wide` (大), `Oh` (驚き)


* **内部ロジック (Implementation):**
* **リップシンク:** `VoiceModule` から来る音量レベル(0.0-1.0)に対し、閾値を設定して口の形状を決定する（例: 0.1以上でOpen, 0.6以上でWide）。
* **自動瞬き:** 内部タイマーで 3.0〜5.0秒ごとにランダムに `Eyes: Closed` 状態を 0.1秒間挿入する。


* **API通信フロー:**
* **アセット生成時のみ通信:** ユーザーが「新しい表情」を追加したい場合に、YAMLプロンプトを送信して画像をダウンロード・保存する。動画再生中は通信しない。



### 4.3 SceneModule (背景・図解)

**責務:** 背景レイヤーの生成と管理、ZStack上の配置制御。

* **使用するAPI:**
* **Google Gemini API (Image Generation)** / `imagen-3.0-generate-001` 等


* **内部ロジック:**
* **レイヤー管理:** 背景画像、前景アイテム（机など）、キャラクターの重なり順を管理。
* **ユーザーアセット:** ユーザーが持っている画像ファイルのインポート機能。


* **API通信フロー:**
* ユーザーが背景プロンプト（例：「近未来のラボ」）を入力した際にAPIをコールし、画像を生成してアセットフォルダに保存する。



### 4.4 TimelineModule (スクリプト管理・指揮)

**責務:** 動画全体の構成データ（絵コンテ）の管理と再生進行。

* **使用するAPI (オプション):**
* **Google Gemini API (Text Generation)** / `gemini-1.5-pro`


* **データ構造 (Struct):**
```swift
struct ScriptBlock: Identifiable {
    var id: UUID
    var text: String          // セリフ
    var voiceAssetID: String? // 音声ファイルID
    var emotion: EmotionTag   // この時の表情
    var backgroundID: String? // この時の背景
}

```


* **内部ロジック:**
* **シーケンサー:** 現在の `ScriptBlock` の音声を再生開始させ、終了通知 (`audioDidFinishPlaying`) を受け取ったら次のブロックへ移行する。


* **API通信フロー (脚本自動生成時):**
* ユーザーのざっくりした指示から、上記の `ScriptBlock` の配列（JSON）をGeminiに生成させる。



### 4.5 ExportModule (動画書き出し)

ご提示いただいた内容は、動画生成の**具体的な実装手順（テクニカルフロー）**そのものですので、**絶対に必要（残すべき）**です。

直前の私の回答（Version 3.1）では、字幕機能の説明に重点を置いたため、動画書き出し部分の記述が少し簡易的になっていました。ご提示いただいた**「AVAssetWriter Flow（1〜6の手順）」**は、CVPixelBufferやCoreGraphicsといった具体的なクラス名が含まれており、実装時に非常に役立つため、これを核として字幕機能を統合するのがベストです。

以下に、ご提示いただいた内容を活かしつつ、字幕・翻訳機能を統合した**「完全版」**を作成しました。

---

# 4.5 ExportModule (動画・字幕書き出し)

**責務:**
メモリ上のプレビュー状態を物理的な `.mp4` ファイルとしてレンダリングし、同時に多言語対応の字幕ファイル (`.srt`) を生成する。

* **使用するAPI:**
* **Google Gemini API (Text Generation)** / `gemini-1.5-flash`
* ※動画エンコード自体はローカルで行うが、字幕の翻訳時にAPIを使用する。


* **内部ロジック A: 動画レンダリング (AVAssetWriter Flow)**
1. **初期化:** `AVAssetWriter` で出力設定（H.264/AAC, 1920x1080, 30fps/60fps）を行う。
2. **フレームループ:** 動画のFPSに従って時間を進める（例: `CMTime` を 1/30秒ずつ加算）。
3. **合成 (Compositing):**
* 各時刻において `AvatarModule`（現在の表情・口パク）と `SceneModule`（背景）に「今の見た目」を問い合わせる。
* `CoreGraphics` コンテキストを使用してこれらを重ね合わせ、1枚の画像バッファ（`CVPixelBuffer`）を生成する。


4. **書き込み:** 生成したバッファを `AVAssetWriterInputPixelBufferAdaptor` 経由で追加する。
5. **音声合成:** `TimelineModule` が持つ各ブロックの音声データを、タイムラインの開始位置に合わせて結合し、オーディオトラックに追加する。
6. **完了:** `finishWriting` を呼び出し、ファイルを保存する。


* **内部ロジック B: 字幕生成 (Subtitle Flow)**
1. **SRT生成:** `TimelineModule` のスクリプト配列を走査し、各セリフの「開始時間」「終了時間」「テキスト」を抽出して、SRTフォーマットのテキストデータを構築する。
2. **翻訳 (Translation):**
* 構築した日本語SRTテキスト全体を、`AIInfra` 経由で Gemini API に送信する。
* プロンプト: *"以下のSRTファイルのタイムコード構造を維持したまま、キャラクターの口調（丁寧、快活など）を反映して英語に翻訳してください。"*


3. **保存:** オリジナルの日本語SRTと、翻訳された英語SRTをテキストファイルとして保存する。


* **出力ファイル構成:**
* `project_name.mp4` (動画本体)
* `project_name.ja.srt` (日本語字幕)
* `project_name.en.srt` (英語字幕)
---

## 5. データ構造

アプリ全体で扱うデータ形式（JSON想定）。

### 5.1 Project File (.koyomi)

```json
{
  "project_name": "マニュアル動画_01",
  "resolution": {"width": 1920, "height": 1080},
  "assets": {
    "character_id": "char_001",
    "backgrounds": ["bg_office.png", "bg_cafe.png"]
  },
  "timeline": [
    {
      "id": 1,
      "text": "みなさんこんにちは、彩瀬こよみです。",
      "emotion": "smile",
      "background_ref": "bg_office.png",
      "voice_file": "voice_001.mp3",
      "duration": 3.5
    },
    {
      "id": 2,
      "text": "今日は新しい経費精算システムについて説明します。",
      "emotion": "serious",
      "background_ref": "bg_office.png",
      "voice_file": "voice_002.mp3",
      "duration": 4.2
    }
  ]
}

```

---

## 6. 非機能要件

### 6.1 API・コスト管理 **【追記】**

* **APIキー管理:**
* 1つのGoogle AI Studio APIキーですべての機能（音声・画像・テキスト）を賄う。
* APIキーは `Secrets.xcconfig` 等で管理し、Gitリポジトリには含めないこと。


* **トークン節約:**
* 生成した音声や画像は必ずローカルストレージにキャッシュし、同一内容での再生成（APIコールの無駄遣い）を防ぐロジックを実装する。



---

**文書終了**